2023-03-21 19:18:26,103 Namespace(GAMMA=0.1, STEPS=[60000, 80000], batch_size=1, co_gt_rev_syn_root='./dataset/train_data/DUTS_class_syn/img_png_seamless_cloning_add_naive_reverse_2/co_gt', co_gt_root='./dataset/train_data/DUTS_class/gt', co_gt_root_coco='./dataset/train_data/CoCo9k/gt', config_file='./config/cosod.yaml', device_id='0', img_rev_syn_root='./dataset/train_data/DUTS_class_syn/img_png_seamless_cloning_add_naive_reverse_2/img', img_root='./dataset/train_data/DUTS_class/img', img_root_coco='./dataset/train_data/CoCo9k/img', img_size=256, img_syn_root='./dataset/train_data/DUTS_class_syn/img_png_seamless_cloning_add_naive/img', lr=0.0001, max_epoches=300, max_num=8, model_name=None, model_root_dir='./checkpoint', num_works=1, save_dir='./prediction', scale_size=288, test_data_root='./dataset/test_data', test_datasets=['CoCA'], test_max_num=25, train_steps=80000, train_w_coco_prob=0.5, warmup_factor=0.001, warmup_iters=1000, warmup_method='linear')
2023-03-21 19:18:26,104 MODEL:
  COFORMER_DECODER:
    DROP_PATH: 0.1
    FEEDFORWARD_DIM: 512
    FFN_EXP: 3
    HIDDEN_DIM: 256
    NUM_HEADS: 8
  DASPP:
    ADAP_CHANNEL: 512
    DILATIONS: [2, 4, 8]
  ENCODER:
    CHANNEL: [64, 128, 256, 512, 512]
    NAME: ['conv1_2', 'conv2_2', 'conv3_3', 'conv4_3', 'conv5_3']
    STRIDE: [1, 2, 4, 8, 8]
  GROUP_ATTENTION:
    CHANNEL: [512]
    DROP_RATE: 0.1
    MSP_SCALES: [1, 3, 6]
    NAME: ['conv5_GA']
    NUM_HEADS: 8
  NAME: CoSOD
  PRETRAINED: 
2023-03-21 19:18:26,180 
    Starting training:
        Train steps: 80000
        Batch size: 1
        Learning rate: 0.0001
        Training size: 356
    
2023-03-21 19:18:26,180 => building model
2023-03-21 19:18:29,776 CoSODNet(
  (encoder): B2_VGG(
    (conv1): Sequential(
      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_1): ReLU(inplace=True)
      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_2): ReLU(inplace=True)
    )
    (conv2): Sequential(
      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_1): ReLU()
      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_2): ReLU()
    )
    (conv3): Sequential(
      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_1): ReLU()
      (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_2): ReLU()
      (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_3): ReLU()
    )
    (conv4): Sequential(
      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_1): ReLU()
      (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_2): ReLU()
      (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_3): ReLU()
    )
    (conv5): Sequential(
      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_1): ReLU()
      (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_2): ReLU()
      (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_3): ReLU()
    )
  )
  (daspp_block): DASPPBlock(
    (adap_layer): Conv2d(
      512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
    (conv_brach_1): _DASPPConvBranch(
      (conv1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv2): Conv2d(
        256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (conv_brach_2): _DASPPConvBranch(
      (conv1): Conv2d(
        704, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv2): Conv2d(
        256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (conv_brach_3): _DASPPConvBranch(
      (conv1): Conv2d(
        896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv2): Conv2d(
        256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (after_daspp): Conv2d(
      1600, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (group_att): GroupAttention(
    (attention): AnyAttention(
      (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm_k): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm_v): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (to_q): Linear(in_features=512, out_features=512, bias=False)
      (to_k): Linear(in_features=512, out_features=512, bias=False)
      (to_v): Linear(in_features=512, out_features=512, bias=False)
      (proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (drop_path): DropPath()
    (msp_block): MultiScalePooling()
    (ffn): MLP(
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU(approximate=none)
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (cosod_former): CoFormer_Decoder(
    (pe_layer): PositionEmbeddingSine()
    (group_pos): Embedding(2, 256)
    (com_pos): Embedding(1, 256)
    (input_proj): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (parse_1): Parse(
      (trans_feat2gr): Trans_Feat2Tokes(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj1): Linear(in_features=256, out_features=256, bias=True)
          (proj2): Linear(in_features=256, out_features=256, bias=True)
          (mlp): Conv1d(256, 256, kernel_size=(1,), stride=(1,), groups=256)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
      )
      (trans_gr2com): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
      (trans_com2gr): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
    )
    (decoder_1): Decoder_Conv(
      (lateral_conv): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (output_conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (parse_2): Parse(
      (trans_feat2gr): Trans_Feat2Tokes(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj1): Linear(in_features=256, out_features=256, bias=True)
          (proj2): Linear(in_features=256, out_features=256, bias=True)
          (mlp): Conv1d(256, 256, kernel_size=(1,), stride=(1,), groups=256)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.10000000149011612, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.10000000149011612, inplace=False)
        (dropout2): Dropout(p=0.10000000149011612, inplace=False)
        (dropout3): Dropout(p=0.10000000149011612, inplace=False)
      )
      (trans_gr2com): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.10000000149011612, inplace=False)
        (dropout2): Dropout(p=0.10000000149011612, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.10000000149011612, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
      (trans_com2gr): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.10000000149011612, inplace=False)
        (dropout2): Dropout(p=0.10000000149011612, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.10000000149011612, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
    )
    (decoder_2): Decoder_Conv(
      (lateral_conv): Conv2d(
        128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (output_conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (group_att_2): GroupAttention(
      (attention): AnyAttention(
        (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (to_q): Linear(in_features=256, out_features=256, bias=False)
        (to_k): Linear(in_features=256, out_features=256, bias=False)
        (to_v): Linear(in_features=256, out_features=256, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
      )
      (drop_path): DropPath()
      (msp_block): MultiScalePooling()
      (ffn): MLP(
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=256, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=256, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (proto_refine_2): Prototype_Refinement(
      (gr_sal_pred): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_2): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_3): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (fuse_co): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (fuse_bg): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (parse_3): Parse(
      (trans_feat2gr): Trans_Feat2Tokes(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj1): Linear(in_features=256, out_features=256, bias=True)
          (proj2): Linear(in_features=256, out_features=256, bias=True)
          (mlp): Conv1d(256, 256, kernel_size=(1,), stride=(1,), groups=256)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.07500000298023224, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.07500000298023224, inplace=False)
        (dropout2): Dropout(p=0.07500000298023224, inplace=False)
        (dropout3): Dropout(p=0.07500000298023224, inplace=False)
      )
      (trans_gr2com): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.07500000298023224, inplace=False)
        (dropout2): Dropout(p=0.07500000298023224, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.07500000298023224, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
      (trans_com2gr): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.07500000298023224, inplace=False)
        (dropout2): Dropout(p=0.07500000298023224, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.07500000298023224, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
    )
    (decoder_3): Decoder_Conv(
      (lateral_conv): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (output_conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (group_att_3): GroupAttention(
      (attention): AnyAttention(
        (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (to_q): Linear(in_features=256, out_features=256, bias=False)
        (to_k): Linear(in_features=256, out_features=256, bias=False)
        (to_v): Linear(in_features=256, out_features=256, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
      )
      (drop_path): DropPath()
      (msp_block): MultiScalePooling()
      (ffn): MLP(
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=256, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=256, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (proto_refine_3): Prototype_Refinement(
      (gr_sal_pred): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_2): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_3): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (fuse_co): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (fuse_bg): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (parse_4): Parse(
      (trans_feat2gr): Trans_Feat2Tokes(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj1): Linear(in_features=256, out_features=256, bias=True)
          (proj2): Linear(in_features=256, out_features=256, bias=True)
          (mlp): Conv1d(256, 256, kernel_size=(1,), stride=(1,), groups=256)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.05000000074505806, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.05000000074505806, inplace=False)
        (dropout2): Dropout(p=0.05000000074505806, inplace=False)
        (dropout3): Dropout(p=0.05000000074505806, inplace=False)
      )
      (trans_gr2com): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.05000000074505806, inplace=False)
        (dropout2): Dropout(p=0.05000000074505806, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.05000000074505806, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
      (trans_com2gr): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.05000000074505806, inplace=False)
        (dropout2): Dropout(p=0.05000000074505806, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.05000000074505806, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
    )
    (decoder_4): Decoder_Conv(
      (lateral_conv): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (output_conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (group_att_4): GroupAttention(
      (attention): AnyAttention(
        (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (to_q): Linear(in_features=256, out_features=256, bias=False)
        (to_k): Linear(in_features=256, out_features=256, bias=False)
        (to_v): Linear(in_features=256, out_features=256, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
      )
      (drop_path): DropPath()
      (msp_block): MultiScalePooling()
      (ffn): MLP(
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=256, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=256, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (proto_refine_4): Prototype_Refinement(
      (gr_sal_pred): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_2): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_3): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (fuse_co): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (fuse_bg): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (parse_5): Parse(
      (trans_feat2gr): Trans_Feat2Tokes(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj1): Linear(in_features=256, out_features=256, bias=True)
          (proj2): Linear(in_features=256, out_features=256, bias=True)
          (mlp): Conv1d(256, 256, kernel_size=(1,), stride=(1,), groups=256)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.02500000037252903, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.02500000037252903, inplace=False)
        (dropout2): Dropout(p=0.02500000037252903, inplace=False)
        (dropout3): Dropout(p=0.02500000037252903, inplace=False)
      )
      (trans_gr2com): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.02500000037252903, inplace=False)
        (dropout2): Dropout(p=0.02500000037252903, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.02500000037252903, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
      (trans_com2gr): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.02500000037252903, inplace=False)
        (dropout2): Dropout(p=0.02500000037252903, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.02500000037252903, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
    )
    (decoder_5): Decoder_Conv(
      (lateral_conv): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (output_conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (group_att_5): GroupAttention(
      (attention): AnyAttention(
        (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (to_q): Linear(in_features=256, out_features=256, bias=False)
        (to_k): Linear(in_features=256, out_features=256, bias=False)
        (to_v): Linear(in_features=256, out_features=256, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
      )
      (drop_path): DropPath()
      (msp_block): MultiScalePooling()
      (ffn): MLP(
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=256, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=256, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (proto_refine_5): Prototype_Refinement(
      (gr_sal_pred): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_2): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_3): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (fuse_co): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (fuse_bg): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (parse_6): Parse(
      (trans_feat2gr): Trans_Feat2Tokes(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj1): Linear(in_features=256, out_features=256, bias=True)
          (proj2): Linear(in_features=256, out_features=256, bias=True)
          (mlp): Conv1d(256, 256, kernel_size=(1,), stride=(1,), groups=256)
        )
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
      )
      (trans_gr2com): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
      (trans_com2gr): Trans_Tokes2Tokes(
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (linear1): Linear(in_features=256, out_features=512, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=512, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (activation): GELU(approximate=none)
      )
    )
    (decoder_6): Decoder_Conv(
      (output_conv): Conv2d(
        512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (proto_refine_6): Prototype_Refinement(
      (gr_sal_pred): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_2): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (com_sal_pred_3): Sal_Pred_with_Tok(
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (fuse_co): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (fuse_bg): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (gr_sal_preds): Sal_Pred_with_Tok(
      (linear1): Linear(in_features=256, out_features=256, bias=True)
      (linear2): Linear(in_features=256, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
)
2023-03-21 19:18:29,788 Starting epoch 1/300.
2023-03-21 19:18:29,788 epoch: 0 ------ lr:1.0000000000000001e-07
2023-03-21 19:18:37,480 Whole iter step:1 - epoch progress:0/300 - total_loss:9.3236 - f_co_bce:0.7137 - f_bg_bce: 0.7542 - f_com_bce: 0.7511 - f_iou: 0.8561 - f_iou_com: 0.8707 - s_co_bce:0.7231 - s_bg_bce: 0.7410 - s_com_bce: 0.7383 - s_co_1_bce: 0.7266 - s_co_2_bce: 0.7266 - s_iou:0.7248 - s_iou_com:0.8585  batch_size: 0.8656041026115417
2023-03-21 19:18:39,175 Whole iter step:2 - epoch progress:0/300 - total_loss:9.1421 - f_co_bce:0.7537 - f_bg_bce: 0.7235 - f_com_bce: 0.7503 - f_iou: 0.8143 - f_iou_com: 0.8242 - s_co_bce:0.7275 - s_bg_bce: 0.7378 - s_com_bce: 0.7313 - s_co_1_bce: 0.7173 - s_co_2_bce: 0.7173 - s_iou:0.7323 - s_iou_com:0.8118  batch_size: 0.8180899620056152
2023-03-21 19:18:40,454 Whole iter step:3 - epoch progress:0/300 - total_loss:8.7815 - f_co_bce:0.7164 - f_bg_bce: 0.7335 - f_com_bce: 0.7433 - f_iou: 0.7283 - f_iou_com: 0.7350 - s_co_bce:0.7235 - s_bg_bce: 0.7375 - s_com_bce: 0.7416 - s_co_1_bce: 0.7346 - s_co_2_bce: 0.7346 - s_iou:0.7327 - s_iou_com:0.7231  batch_size: 0.7319236993789673
2023-03-21 19:18:41,409 Whole iter step:4 - epoch progress:0/300 - total_loss:9.4559 - f_co_bce:0.7354 - f_bg_bce: 0.7168 - f_com_bce: 0.7327 - f_iou: 0.9034 - f_iou_com: 0.9023 - s_co_bce:0.7293 - s_bg_bce: 0.7435 - s_com_bce: 0.7353 - s_co_1_bce: 0.7218 - s_co_2_bce: 0.7218 - s_iou:0.7347 - s_iou_com:0.8980  batch_size: 0.9027292132377625
2023-03-21 19:18:42,573 Whole iter step:5 - epoch progress:0/300 - total_loss:9.5971 - f_co_bce:0.7310 - f_bg_bce: 0.7161 - f_com_bce: 0.7578 - f_iou: 0.9280 - f_iou_com: 0.9366 - s_co_bce:0.7339 - s_bg_bce: 0.7336 - s_com_bce: 0.7383 - s_co_1_bce: 0.7195 - s_co_2_bce: 0.7195 - s_iou:0.7444 - s_iou_com:0.9281  batch_size: 0.9298699498176575
